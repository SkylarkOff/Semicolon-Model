{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10839219,"sourceType":"datasetVersion","datasetId":6731093}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-25T20:34:47.294325Z","iopub.execute_input":"2025-03-25T20:34:47.294781Z","iopub.status.idle":"2025-03-25T20:34:47.831631Z","shell.execute_reply.started":"2025-03-25T20:34:47.294745Z","shell.execute_reply":"2025-03-25T20:34:47.830185Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/data-baru/Normalisasi_data.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom tabulate import tabulate  # Untuk tampilan tabel lebih rapi\n\n# **1. Load Dataset**\ndf = pd.read_csv(\"/kaggle/input/data-baru/Normalisasi_data.csv\")\n\n# **2. Preprocessing**\ndf[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\ndf[\"hour\"] = df[\"datetime\"].dt.hour  \ndf[\"weekday\"] = df[\"datetime\"].dt.weekday  \ndf[\"day\"] = df[\"datetime\"].dt.day  \ndf[\"month\"] = df[\"datetime\"].dt.month  \ndf[\"year\"] = df[\"datetime\"].dt.year  \n\n# **Konversi Participant ID (pid) ke Kode Numerik**\ngroup_column = \"pid\"\ndf[group_column] = df[group_column].astype(\"category\").cat.codes  \n\nfeature_columns = [\n    \"AU24\", \"hour\", \"weekday\", \"day\", \"month\", \"Eye_Open_Avg\", \"Facial_Structure\", \n    \"AU23\", \"AU01\", \"AU07\", \"AU02\", \"AU_Smile\", \"AU10\", \"AU14\"\n]\ntarget_column = \"depression_episode\"\n\nX = df[feature_columns]\ny = df[target_column]\n\n# **3. Normalisasi Data**\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\n\n# **4. Custom Dataset Class**\nclass DepressionDataset(Dataset):\n    def __init__(self, X, y, pids):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y.values, dtype=torch.float32).unsqueeze(1)\n        self.pids = torch.tensor(pids.to_numpy(), dtype=torch.int32)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx], self.pids[idx]\n\n# **5. Inisialisasi Model Neural Network**\nclass DepressionMLP(nn.Module):\n    def __init__(self, input_size):\n        super(DepressionMLP, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_size, 64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(32, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n# **6. Stratified K-Fold Cross-Validation**\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nfold_results = []\nparticipant_results = []  # Menyimpan hasil evaluasi per participant\ninput_dim = X_scaled.shape[1]\n\nfor fold, (train_idx, test_idx) in enumerate(kf.split(X_scaled, y)):\n    print(f\"\\nüîπ Fold {fold+1}/5\")\n\n    # **Bagi dataset berdasarkan index dari K-Fold**\n    train_dataset = Subset(DepressionDataset(X_scaled, y, df[group_column]), train_idx)\n    test_dataset = Subset(DepressionDataset(X_scaled, y, df[group_column]), test_idx)\n\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n    # **Inisialisasi ulang model setiap fold**\n    model = DepressionMLP(input_dim).to(device)\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # **Training Loop**\n    num_epochs = 25\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n\n        for X_batch, y_batch, _ in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\n            optimizer.zero_grad()\n            y_pred = model(X_batch)\n            loss = criterion(y_pred, y_batch)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        # **Evaluasi Setiap Epoch**\n        model.eval()\n        y_pred_list, y_true_list = [], []\n\n        with torch.no_grad():\n            for X_batch, y_batch, _ in test_loader:\n                X_batch = X_batch.to(device)\n                y_pred = model(X_batch).cpu().numpy()\n                y_pred_list.extend(y_pred)\n                y_true_list.extend(y_batch.numpy())\n\n        y_pred_list = np.array(y_pred_list).flatten()\n        y_pred_labels = (y_pred_list > 0.5).astype(int)\n\n        acc = accuracy_score(y_true_list, y_pred_labels)\n        precision = precision_score(y_true_list, y_pred_labels, zero_division=0)\n        recall = recall_score(y_true_list, y_pred_labels, zero_division=0)\n        f1 = f1_score(y_true_list, y_pred_labels, zero_division=0)\n        auc = roc_auc_score(y_true_list, y_pred_list) if len(np.unique(y_true_list)) > 1 else np.nan\n\n        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss/len(train_loader):.4f} \"\n              f\"- Accuracy: {acc:.4f} - Precision: {precision:.4f} - Recall: {recall:.4f} \"\n              f\"- F1 Score: {f1:.4f} - AUC: {auc:.4f}\")\n\n    # **Evaluasi Per Participant**\n    model.eval()\n    y_pred_list, y_true_list, pid_list = [], [], []\n\n    with torch.no_grad():\n        for X_batch, y_batch, pids in test_loader:\n            X_batch = X_batch.to(device)\n            y_pred = model(X_batch).cpu().numpy()\n            y_pred_list.extend(y_pred)\n            y_true_list.extend(y_batch.numpy())\n            pid_list.extend(pids.numpy())\n\n    y_pred_list = np.array(y_pred_list).flatten()\n    y_pred_labels = (y_pred_list > 0.5).astype(int)\n\n    unique_pids = np.unique(pid_list)\n    for pid in unique_pids:\n        mask = np.array(pid_list) == pid\n        y_true_pid = np.array(y_true_list)[mask]\n        y_pred_pid = np.array(y_pred_labels)[mask]\n        y_score_pid = np.array(y_pred_list)[mask]\n\n        if len(np.unique(y_true_pid)) > 1:\n            auc_score = roc_auc_score(y_true_pid, y_score_pid)\n        else:\n            auc_score = np.nan\n\n        participant_results.append({\n            \"Participant\": pid,\n            \"Fold\": fold + 1,\n            \"Accuracy\": accuracy_score(y_true_pid, y_pred_pid),\n            \"Precision\": precision_score(y_true_pid, y_pred_pid, zero_division=0),\n            \"Recall\": recall_score(y_true_pid, y_pred_pid, zero_division=0),\n            \"F1 Score\": f1_score(y_true_pid, y_pred_pid, zero_division=0),\n            \"AUC\": auc_score\n        })\n\n# **7. Simpan & Tampilkan Hasil Evaluasi**\nparticipant_df = pd.DataFrame(participant_results)\n\n# **Hitung Rata-rata Hasil Evaluasi**\navg_results = {\n    \"Participant\": \"Average\",\n    \"Fold\": \"-\",\n    \"Accuracy\": participant_df[\"Accuracy\"].mean(),\n    \"Precision\": participant_df[\"Precision\"].mean(),\n    \"Recall\": participant_df[\"Recall\"].mean(),\n    \"F1 Score\": participant_df[\"F1 Score\"].mean(),\n    \"AUC\": participant_df[\"AUC\"].mean()\n}\n\n# **Gabungkan Hasil Individual dengan Rata-rata**\nparticipant_df = pd.concat([participant_df, pd.DataFrame([avg_results])], ignore_index=True)\n\n# **Tampilkan Hasil Evaluasi dengan Rata-rata**\nprint(\"\\nüîç Hasil Evaluasi Per Participant + Average:\")\nprint(tabulate(participant_df, headers=\"keys\", tablefmt=\"grid\", floatfmt=\".4f\"))\n\n# **Simpan Hasil Evaluasi ke CSV**\nparticipant_df.to_csv(\"hasil_evaluasi_per_participant.csv\", index=False)\nprint(\"‚úÖ Hasil evaluasi per participant disimpan dalam 'hasil_evaluasi_per_participant.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T22:50:22.729453Z","iopub.execute_input":"2025-03-25T22:50:22.729916Z","iopub.status.idle":"2025-03-26T00:35:17.702409Z","shell.execute_reply.started":"2025-03-25T22:50:22.729885Z","shell.execute_reply":"2025-03-26T00:35:17.701318Z"}},"outputs":[{"name":"stdout","text":"\nüîπ Fold 1/5\nEpoch 1/25 - Loss: 0.3062 - Accuracy: 0.8885 - Precision: 0.8924 - Recall: 0.8500 - F1 Score: 0.8707 - AUC: 0.9623\nEpoch 2/25 - Loss: 0.2679 - Accuracy: 0.8940 - Precision: 0.8904 - Recall: 0.8666 - F1 Score: 0.8783 - AUC: 0.9669\nEpoch 3/25 - Loss: 0.2583 - Accuracy: 0.8986 - Precision: 0.8937 - Recall: 0.8745 - F1 Score: 0.8840 - AUC: 0.9688\nEpoch 4/25 - Loss: 0.2526 - Accuracy: 0.9047 - Precision: 0.9148 - Recall: 0.8648 - F1 Score: 0.8891 - AUC: 0.9708\nEpoch 5/25 - Loss: 0.2482 - Accuracy: 0.9067 - Precision: 0.9115 - Recall: 0.8738 - F1 Score: 0.8922 - AUC: 0.9722\nEpoch 6/25 - Loss: 0.2454 - Accuracy: 0.9079 - Precision: 0.9054 - Recall: 0.8839 - F1 Score: 0.8945 - AUC: 0.9724\nEpoch 7/25 - Loss: 0.2436 - Accuracy: 0.9068 - Precision: 0.9079 - Recall: 0.8782 - F1 Score: 0.8928 - AUC: 0.9731\nEpoch 8/25 - Loss: 0.2409 - Accuracy: 0.9073 - Precision: 0.9011 - Recall: 0.8875 - F1 Score: 0.8942 - AUC: 0.9725\nEpoch 9/25 - Loss: 0.2395 - Accuracy: 0.9101 - Precision: 0.9103 - Recall: 0.8835 - F1 Score: 0.8967 - AUC: 0.9735\nEpoch 10/25 - Loss: 0.2388 - Accuracy: 0.9149 - Precision: 0.9199 - Recall: 0.8843 - F1 Score: 0.9017 - AUC: 0.9749\nEpoch 11/25 - Loss: 0.2372 - Accuracy: 0.9134 - Precision: 0.9091 - Recall: 0.8933 - F1 Score: 0.9012 - AUC: 0.9743\nEpoch 12/25 - Loss: 0.2362 - Accuracy: 0.9127 - Precision: 0.9287 - Recall: 0.8691 - F1 Score: 0.8979 - AUC: 0.9750\nEpoch 13/25 - Loss: 0.2350 - Accuracy: 0.9137 - Precision: 0.9375 - Recall: 0.8621 - F1 Score: 0.8982 - AUC: 0.9754\nEpoch 14/25 - Loss: 0.2341 - Accuracy: 0.9158 - Precision: 0.9254 - Recall: 0.8804 - F1 Score: 0.9023 - AUC: 0.9754\nEpoch 15/25 - Loss: 0.2330 - Accuracy: 0.9189 - Precision: 0.9141 - Recall: 0.9011 - F1 Score: 0.9076 - AUC: 0.9764\nEpoch 16/25 - Loss: 0.2326 - Accuracy: 0.9153 - Precision: 0.9320 - Recall: 0.8719 - F1 Score: 0.9010 - AUC: 0.9757\nEpoch 17/25 - Loss: 0.2321 - Accuracy: 0.9137 - Precision: 0.9298 - Recall: 0.8703 - F1 Score: 0.8990 - AUC: 0.9753\nEpoch 18/25 - Loss: 0.2307 - Accuracy: 0.9152 - Precision: 0.9092 - Recall: 0.8976 - F1 Score: 0.9034 - AUC: 0.9756\nEpoch 19/25 - Loss: 0.2313 - Accuracy: 0.9207 - Precision: 0.9274 - Recall: 0.8901 - F1 Score: 0.9084 - AUC: 0.9772\nEpoch 20/25 - Loss: 0.2297 - Accuracy: 0.9185 - Precision: 0.9218 - Recall: 0.8912 - F1 Score: 0.9062 - AUC: 0.9765\nEpoch 21/25 - Loss: 0.2301 - Accuracy: 0.9178 - Precision: 0.9211 - Recall: 0.8901 - F1 Score: 0.9053 - AUC: 0.9770\nEpoch 22/25 - Loss: 0.2294 - Accuracy: 0.9201 - Precision: 0.9269 - Recall: 0.8892 - F1 Score: 0.9076 - AUC: 0.9774\nEpoch 23/25 - Loss: 0.2294 - Accuracy: 0.9205 - Precision: 0.9260 - Recall: 0.8914 - F1 Score: 0.9084 - AUC: 0.9772\nEpoch 24/25 - Loss: 0.2293 - Accuracy: 0.9188 - Precision: 0.9320 - Recall: 0.8803 - F1 Score: 0.9054 - AUC: 0.9765\nEpoch 25/25 - Loss: 0.2279 - Accuracy: 0.9178 - Precision: 0.9239 - Recall: 0.8870 - F1 Score: 0.9051 - AUC: 0.9763\n\nüîπ Fold 2/5\nEpoch 1/25 - Loss: 0.3115 - Accuracy: 0.8930 - Precision: 0.8795 - Recall: 0.8780 - F1 Score: 0.8788 - AUC: 0.9577\nEpoch 2/25 - Loss: 0.2774 - Accuracy: 0.8970 - Precision: 0.8839 - Recall: 0.8827 - F1 Score: 0.8833 - AUC: 0.9628\nEpoch 3/25 - Loss: 0.2640 - Accuracy: 0.9004 - Precision: 0.9091 - Recall: 0.8605 - F1 Score: 0.8841 - AUC: 0.9681\nEpoch 4/25 - Loss: 0.2576 - Accuracy: 0.8993 - Precision: 0.8940 - Recall: 0.8759 - F1 Score: 0.8848 - AUC: 0.9687\nEpoch 5/25 - Loss: 0.2527 - Accuracy: 0.9003 - Precision: 0.8902 - Recall: 0.8833 - F1 Score: 0.8867 - AUC: 0.9705\nEpoch 6/25 - Loss: 0.2501 - Accuracy: 0.9032 - Precision: 0.9014 - Recall: 0.8769 - F1 Score: 0.8890 - AUC: 0.9708\nEpoch 7/25 - Loss: 0.2476 - Accuracy: 0.9003 - Precision: 0.8913 - Recall: 0.8817 - F1 Score: 0.8865 - AUC: 0.9712\nEpoch 8/25 - Loss: 0.2452 - Accuracy: 0.9054 - Precision: 0.9103 - Recall: 0.8717 - F1 Score: 0.8905 - AUC: 0.9726\nEpoch 9/25 - Loss: 0.2435 - Accuracy: 0.9053 - Precision: 0.9076 - Recall: 0.8747 - F1 Score: 0.8908 - AUC: 0.9721\nEpoch 10/25 - Loss: 0.2420 - Accuracy: 0.9068 - Precision: 0.9253 - Recall: 0.8583 - F1 Score: 0.8905 - AUC: 0.9719\nEpoch 11/25 - Loss: 0.2410 - Accuracy: 0.9044 - Precision: 0.9102 - Recall: 0.8694 - F1 Score: 0.8894 - AUC: 0.9726\nEpoch 12/25 - Loss: 0.2404 - Accuracy: 0.9078 - Precision: 0.9157 - Recall: 0.8715 - F1 Score: 0.8931 - AUC: 0.9740\nEpoch 13/25 - Loss: 0.2397 - Accuracy: 0.9060 - Precision: 0.9120 - Recall: 0.8713 - F1 Score: 0.8912 - AUC: 0.9730\nEpoch 14/25 - Loss: 0.2386 - Accuracy: 0.9091 - Precision: 0.9200 - Recall: 0.8698 - F1 Score: 0.8942 - AUC: 0.9744\nEpoch 15/25 - Loss: 0.2377 - Accuracy: 0.9097 - Precision: 0.9251 - Recall: 0.8656 - F1 Score: 0.8944 - AUC: 0.9735\nEpoch 16/25 - Loss: 0.2374 - Accuracy: 0.9074 - Precision: 0.9203 - Recall: 0.8654 - F1 Score: 0.8920 - AUC: 0.9743\nEpoch 17/25 - Loss: 0.2365 - Accuracy: 0.9088 - Precision: 0.9151 - Recall: 0.8747 - F1 Score: 0.8945 - AUC: 0.9735\nEpoch 18/25 - Loss: 0.2359 - Accuracy: 0.9074 - Precision: 0.9140 - Recall: 0.8725 - F1 Score: 0.8928 - AUC: 0.9733\nEpoch 19/25 - Loss: 0.2356 - Accuracy: 0.9120 - Precision: 0.9227 - Recall: 0.8741 - F1 Score: 0.8977 - AUC: 0.9754\nEpoch 20/25 - Loss: 0.2350 - Accuracy: 0.9086 - Precision: 0.9154 - Recall: 0.8739 - F1 Score: 0.8942 - AUC: 0.9743\nEpoch 21/25 - Loss: 0.2349 - Accuracy: 0.9096 - Precision: 0.9164 - Recall: 0.8752 - F1 Score: 0.8953 - AUC: 0.9747\nEpoch 22/25 - Loss: 0.2343 - Accuracy: 0.9042 - Precision: 0.9121 - Recall: 0.8666 - F1 Score: 0.8888 - AUC: 0.9735\nEpoch 23/25 - Loss: 0.2335 - Accuracy: 0.9121 - Precision: 0.9295 - Recall: 0.8668 - F1 Score: 0.8970 - AUC: 0.9746\nEpoch 24/25 - Loss: 0.2337 - Accuracy: 0.9098 - Precision: 0.9078 - Recall: 0.8859 - F1 Score: 0.8967 - AUC: 0.9747\nEpoch 25/25 - Loss: 0.2332 - Accuracy: 0.9135 - Precision: 0.9188 - Recall: 0.8820 - F1 Score: 0.9000 - AUC: 0.9753\n\nüîπ Fold 3/5\nEpoch 1/25 - Loss: 0.3127 - Accuracy: 0.8908 - Precision: 0.8775 - Recall: 0.8750 - F1 Score: 0.8762 - AUC: 0.9588\nEpoch 2/25 - Loss: 0.2796 - Accuracy: 0.8926 - Precision: 0.8915 - Recall: 0.8619 - F1 Score: 0.8764 - AUC: 0.9625\nEpoch 3/25 - Loss: 0.2674 - Accuracy: 0.8997 - Precision: 0.8988 - Recall: 0.8709 - F1 Score: 0.8846 - AUC: 0.9681\nEpoch 4/25 - Loss: 0.2602 - Accuracy: 0.9012 - Precision: 0.8975 - Recall: 0.8764 - F1 Score: 0.8868 - AUC: 0.9701\nEpoch 5/25 - Loss: 0.2568 - Accuracy: 0.9044 - Precision: 0.9173 - Recall: 0.8612 - F1 Score: 0.8884 - AUC: 0.9708\nEpoch 6/25 - Loss: 0.2537 - Accuracy: 0.9031 - Precision: 0.9143 - Recall: 0.8612 - F1 Score: 0.8870 - AUC: 0.9710\nEpoch 7/25 - Loss: 0.2510 - Accuracy: 0.9021 - Precision: 0.9094 - Recall: 0.8646 - F1 Score: 0.8864 - AUC: 0.9709\nEpoch 8/25 - Loss: 0.2477 - Accuracy: 0.9036 - Precision: 0.9219 - Recall: 0.8540 - F1 Score: 0.8867 - AUC: 0.9713\nEpoch 9/25 - Loss: 0.2447 - Accuracy: 0.9047 - Precision: 0.8948 - Recall: 0.8888 - F1 Score: 0.8918 - AUC: 0.9723\nEpoch 10/25 - Loss: 0.2433 - Accuracy: 0.9071 - Precision: 0.9137 - Recall: 0.8719 - F1 Score: 0.8924 - AUC: 0.9730\nEpoch 11/25 - Loss: 0.2415 - Accuracy: 0.9099 - Precision: 0.9212 - Recall: 0.8705 - F1 Score: 0.8951 - AUC: 0.9736\nEpoch 12/25 - Loss: 0.2402 - Accuracy: 0.9058 - Precision: 0.9126 - Recall: 0.8700 - F1 Score: 0.8908 - AUC: 0.9733\nEpoch 13/25 - Loss: 0.2396 - Accuracy: 0.9098 - Precision: 0.9096 - Recall: 0.8837 - F1 Score: 0.8964 - AUC: 0.9745\nEpoch 14/25 - Loss: 0.2381 - Accuracy: 0.9090 - Precision: 0.9078 - Recall: 0.8837 - F1 Score: 0.8956 - AUC: 0.9740\nEpoch 15/25 - Loss: 0.2374 - Accuracy: 0.9079 - Precision: 0.9088 - Recall: 0.8796 - F1 Score: 0.8940 - AUC: 0.9741\nEpoch 16/25 - Loss: 0.2374 - Accuracy: 0.9147 - Precision: 0.9265 - Recall: 0.8764 - F1 Score: 0.9008 - AUC: 0.9750\nEpoch 17/25 - Loss: 0.2356 - Accuracy: 0.9140 - Precision: 0.9177 - Recall: 0.8846 - F1 Score: 0.9008 - AUC: 0.9752\nEpoch 18/25 - Loss: 0.2364 - Accuracy: 0.9126 - Precision: 0.9280 - Recall: 0.8696 - F1 Score: 0.8979 - AUC: 0.9757\nEpoch 19/25 - Loss: 0.2353 - Accuracy: 0.9142 - Precision: 0.9325 - Recall: 0.8687 - F1 Score: 0.8995 - AUC: 0.9754\nEpoch 20/25 - Loss: 0.2350 - Accuracy: 0.9137 - Precision: 0.9297 - Recall: 0.8704 - F1 Score: 0.8991 - AUC: 0.9759\nEpoch 21/25 - Loss: 0.2346 - Accuracy: 0.9153 - Precision: 0.9165 - Recall: 0.8893 - F1 Score: 0.9027 - AUC: 0.9760\nEpoch 22/25 - Loss: 0.2340 - Accuracy: 0.9153 - Precision: 0.9363 - Recall: 0.8673 - F1 Score: 0.9005 - AUC: 0.9766\nEpoch 23/25 - Loss: 0.2340 - Accuracy: 0.9144 - Precision: 0.9251 - Recall: 0.8772 - F1 Score: 0.9005 - AUC: 0.9761\nEpoch 24/25 - Loss: 0.2336 - Accuracy: 0.9149 - Precision: 0.9267 - Recall: 0.8766 - F1 Score: 0.9009 - AUC: 0.9757\nEpoch 25/25 - Loss: 0.2329 - Accuracy: 0.9134 - Precision: 0.9336 - Recall: 0.8656 - F1 Score: 0.8983 - AUC: 0.9765\n\nüîπ Fold 4/5\nEpoch 1/25 - Loss: 0.3103 - Accuracy: 0.8926 - Precision: 0.8923 - Recall: 0.8608 - F1 Score: 0.8763 - AUC: 0.9606\nEpoch 2/25 - Loss: 0.2745 - Accuracy: 0.8926 - Precision: 0.8735 - Recall: 0.8850 - F1 Score: 0.8792 - AUC: 0.9650\nEpoch 3/25 - Loss: 0.2654 - Accuracy: 0.8962 - Precision: 0.9153 - Recall: 0.8431 - F1 Score: 0.8777 - AUC: 0.9666\nEpoch 4/25 - Loss: 0.2595 - Accuracy: 0.8978 - Precision: 0.9072 - Recall: 0.8563 - F1 Score: 0.8810 - AUC: 0.9684\nEpoch 5/25 - Loss: 0.2562 - Accuracy: 0.8991 - Precision: 0.8981 - Recall: 0.8703 - F1 Score: 0.8840 - AUC: 0.9692\nEpoch 6/25 - Loss: 0.2536 - Accuracy: 0.9003 - Precision: 0.9010 - Recall: 0.8697 - F1 Score: 0.8851 - AUC: 0.9704\nEpoch 7/25 - Loss: 0.2512 - Accuracy: 0.9031 - Precision: 0.9077 - Recall: 0.8691 - F1 Score: 0.8880 - AUC: 0.9717\nEpoch 8/25 - Loss: 0.2486 - Accuracy: 0.9040 - Precision: 0.9133 - Recall: 0.8647 - F1 Score: 0.8883 - AUC: 0.9717\nEpoch 9/25 - Loss: 0.2469 - Accuracy: 0.9045 - Precision: 0.9076 - Recall: 0.8728 - F1 Score: 0.8898 - AUC: 0.9720\nEpoch 10/25 - Loss: 0.2455 - Accuracy: 0.9035 - Precision: 0.9176 - Recall: 0.8587 - F1 Score: 0.8872 - AUC: 0.9720\nEpoch 11/25 - Loss: 0.2446 - Accuracy: 0.9040 - Precision: 0.8989 - Recall: 0.8819 - F1 Score: 0.8903 - AUC: 0.9727\nEpoch 12/25 - Loss: 0.2434 - Accuracy: 0.9048 - Precision: 0.9162 - Recall: 0.8635 - F1 Score: 0.8891 - AUC: 0.9727\nEpoch 13/25 - Loss: 0.2423 - Accuracy: 0.9006 - Precision: 0.9139 - Recall: 0.8557 - F1 Score: 0.8838 - AUC: 0.9717\nEpoch 14/25 - Loss: 0.2420 - Accuracy: 0.9009 - Precision: 0.9016 - Recall: 0.8706 - F1 Score: 0.8858 - AUC: 0.9727\nEpoch 15/25 - Loss: 0.2401 - Accuracy: 0.9076 - Precision: 0.9201 - Recall: 0.8660 - F1 Score: 0.8922 - AUC: 0.9735\nEpoch 16/25 - Loss: 0.2396 - Accuracy: 0.9039 - Precision: 0.9161 - Recall: 0.8612 - F1 Score: 0.8878 - AUC: 0.9728\nEpoch 17/25 - Loss: 0.2393 - Accuracy: 0.9058 - Precision: 0.9204 - Recall: 0.8613 - F1 Score: 0.8899 - AUC: 0.9735\nEpoch 18/25 - Loss: 0.2384 - Accuracy: 0.9041 - Precision: 0.9061 - Recall: 0.8733 - F1 Score: 0.8894 - AUC: 0.9732\nEpoch 19/25 - Loss: 0.2371 - Accuracy: 0.9066 - Precision: 0.9063 - Recall: 0.8795 - F1 Score: 0.8927 - AUC: 0.9741\nEpoch 20/25 - Loss: 0.2370 - Accuracy: 0.9046 - Precision: 0.9087 - Recall: 0.8716 - F1 Score: 0.8898 - AUC: 0.9735\nEpoch 21/25 - Loss: 0.2369 - Accuracy: 0.9070 - Precision: 0.9205 - Recall: 0.8642 - F1 Score: 0.8914 - AUC: 0.9740\nEpoch 22/25 - Loss: 0.2359 - Accuracy: 0.9046 - Precision: 0.9024 - Recall: 0.8792 - F1 Score: 0.8907 - AUC: 0.9740\nEpoch 23/25 - Loss: 0.2351 - Accuracy: 0.9071 - Precision: 0.9120 - Recall: 0.8739 - F1 Score: 0.8926 - AUC: 0.9744\nEpoch 24/25 - Loss: 0.2355 - Accuracy: 0.9093 - Precision: 0.9157 - Recall: 0.8752 - F1 Score: 0.8950 - AUC: 0.9742\nEpoch 25/25 - Loss: 0.2344 - Accuracy: 0.9099 - Precision: 0.9185 - Recall: 0.8736 - F1 Score: 0.8955 - AUC: 0.9757\n\nüîπ Fold 5/5\nEpoch 1/25 - Loss: 0.3063 - Accuracy: 0.8914 - Precision: 0.8641 - Recall: 0.8948 - F1 Score: 0.8792 - AUC: 0.9612\nEpoch 2/25 - Loss: 0.2710 - Accuracy: 0.8958 - Precision: 0.9087 - Recall: 0.8495 - F1 Score: 0.8781 - AUC: 0.9642\nEpoch 3/25 - Loss: 0.2612 - Accuracy: 0.8974 - Precision: 0.9186 - Recall: 0.8422 - F1 Score: 0.8788 - AUC: 0.9666\nEpoch 4/25 - Loss: 0.2557 - Accuracy: 0.8986 - Precision: 0.8948 - Recall: 0.8731 - F1 Score: 0.8838 - AUC: 0.9689\nEpoch 5/25 - Loss: 0.2520 - Accuracy: 0.8990 - Precision: 0.8946 - Recall: 0.8744 - F1 Score: 0.8844 - AUC: 0.9696\nEpoch 6/25 - Loss: 0.2496 - Accuracy: 0.9050 - Precision: 0.9112 - Recall: 0.8697 - F1 Score: 0.8900 - AUC: 0.9709\nEpoch 7/25 - Loss: 0.2475 - Accuracy: 0.9024 - Precision: 0.9057 - Recall: 0.8696 - F1 Score: 0.8873 - AUC: 0.9706\nEpoch 8/25 - Loss: 0.2460 - Accuracy: 0.9023 - Precision: 0.8996 - Recall: 0.8767 - F1 Score: 0.8880 - AUC: 0.9716\nEpoch 9/25 - Loss: 0.2446 - Accuracy: 0.9027 - Precision: 0.8961 - Recall: 0.8820 - F1 Score: 0.8890 - AUC: 0.9698\nEpoch 10/25 - Loss: 0.2432 - Accuracy: 0.9055 - Precision: 0.9093 - Recall: 0.8730 - F1 Score: 0.8908 - AUC: 0.9719\nEpoch 11/25 - Loss: 0.2417 - Accuracy: 0.9076 - Precision: 0.9183 - Recall: 0.8680 - F1 Score: 0.8924 - AUC: 0.9725\nEpoch 12/25 - Loss: 0.2411 - Accuracy: 0.9075 - Precision: 0.9136 - Recall: 0.8732 - F1 Score: 0.8930 - AUC: 0.9726\nEpoch 13/25 - Loss: 0.2404 - Accuracy: 0.9067 - Precision: 0.9082 - Recall: 0.8776 - F1 Score: 0.8926 - AUC: 0.9730\nEpoch 14/25 - Loss: 0.2397 - Accuracy: 0.9087 - Precision: 0.9144 - Recall: 0.8752 - F1 Score: 0.8944 - AUC: 0.9732\nEpoch 15/25 - Loss: 0.2383 - Accuracy: 0.9060 - Precision: 0.9129 - Recall: 0.8702 - F1 Score: 0.8910 - AUC: 0.9733\nEpoch 16/25 - Loss: 0.2384 - Accuracy: 0.9086 - Precision: 0.9220 - Recall: 0.8663 - F1 Score: 0.8933 - AUC: 0.9739\nEpoch 17/25 - Loss: 0.2369 - Accuracy: 0.9056 - Precision: 0.9187 - Recall: 0.8627 - F1 Score: 0.8898 - AUC: 0.9726\nEpoch 18/25 - Loss: 0.2358 - Accuracy: 0.9067 - Precision: 0.9171 - Recall: 0.8672 - F1 Score: 0.8915 - AUC: 0.9741\nEpoch 19/25 - Loss: 0.2357 - Accuracy: 0.9098 - Precision: 0.9198 - Recall: 0.8717 - F1 Score: 0.8951 - AUC: 0.9747\nEpoch 20/25 - Loss: 0.2350 - Accuracy: 0.9099 - Precision: 0.9141 - Recall: 0.8785 - F1 Score: 0.8959 - AUC: 0.9749\nEpoch 21/25 - Loss: 0.2345 - Accuracy: 0.9078 - Precision: 0.9125 - Recall: 0.8752 - F1 Score: 0.8935 - AUC: 0.9748\nEpoch 22/25 - Loss: 0.2341 - Accuracy: 0.9089 - Precision: 0.9306 - Recall: 0.8577 - F1 Score: 0.8927 - AUC: 0.9747\nEpoch 23/25 - Loss: 0.2339 - Accuracy: 0.9069 - Precision: 0.9135 - Recall: 0.8719 - F1 Score: 0.8922 - AUC: 0.9740\nEpoch 24/25 - Loss: 0.2338 - Accuracy: 0.9097 - Precision: 0.9156 - Recall: 0.8763 - F1 Score: 0.8955 - AUC: 0.9752\nEpoch 25/25 - Loss: 0.2329 - Accuracy: 0.9109 - Precision: 0.9239 - Recall: 0.8699 - F1 Score: 0.8961 - AUC: 0.9755\n\nüîç Hasil Evaluasi Per Participant + Average:\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|     | Participant   | Fold   |   Accuracy |   Precision |   Recall |   F1 Score |      AUC |\n+=====+===============+========+============+=============+==========+============+==========+\n|   0 | 0             | 1      |     0.9744 |      0.9957 |   0.9346 |     0.9642 |   0.9981 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|   1 | 1             | 1      |     0.8925 |      1.0000 |   0.7907 |     0.8831 |   0.9999 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|   2 | 2             | 1      |     0.8686 |      0.7766 |   0.7716 |     0.7741 |   0.9537 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|   3 | 3             | 1      |     0.9962 |      1.0000 |   0.8994 |     0.9470 |   0.9999 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|   4 | 4             | 1      |     0.1311 |      1.0000 |   0.0983 |     0.1790 |   0.9477 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|   5 | 5             | 1      |     0.9415 |      0.9163 |   0.9399 |     0.9279 |   0.9884 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|   6 | 6             | 1      |     0.9400 |      0.9512 |   0.9064 |     0.9283 |   0.9908 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|   7 | 7             | 1      |     1.0000 |      0.0000 |   0.0000 |     0.0000 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|   8 | 8             | 1      |     0.9352 |      0.9789 |   0.8993 |     0.9374 |   0.9938 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|   9 | 9             | 1      |     0.9988 |      0.9975 |   1.0000 |     0.9987 |   1.0000 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  10 | 10            | 1      |     0.9169 |      0.9954 |   0.8258 |     0.9027 |   0.9838 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  11 | 11            | 1      |     0.9402 |      0.9745 |   0.8937 |     0.9324 |   0.9933 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  12 | 12            | 1      |     0.9747 |      0.9643 |   0.9723 |     0.9683 |   0.9967 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  13 | 13            | 1      |     0.9842 |      0.9475 |   0.9998 |     0.9729 |   0.9993 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  14 | 14            | 1      |     1.0000 |      1.0000 |   1.0000 |     1.0000 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  15 | 15            | 1      |     1.0000 |      1.0000 |   1.0000 |     1.0000 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  16 | 16            | 1      |     0.9637 |      1.0000 |   0.9637 |     0.9815 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  17 | 17            | 1      |     0.9877 |      0.9801 |   0.9969 |     0.9884 |   0.9992 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  18 | 18            | 1      |     0.9739 |      0.9800 |   0.9701 |     0.9750 |   0.9972 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  19 | 19            | 1      |     0.8822 |      0.6961 |   1.0000 |     0.8208 |   0.9963 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  20 | 20            | 1      |     0.9146 |      0.8387 |   0.9984 |     0.9116 |   0.9917 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  21 | 21            | 1      |     0.1795 |      1.0000 |   0.1795 |     0.3044 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  22 | 22            | 1      |     0.8363 |      0.7608 |   0.9453 |     0.8431 |   0.9493 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  23 | 23            | 1      |     0.9781 |      0.9831 |   0.9947 |     0.9889 |   0.9922 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  24 | 24            | 1      |     0.6791 |      0.4697 |   0.9990 |     0.6390 |   0.9446 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  25 | 0             | 2      |     0.9779 |      1.0000 |   0.9394 |     0.9687 |   0.9991 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  26 | 1             | 2      |     0.9152 |      1.0000 |   0.8399 |     0.9130 |   0.9998 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  27 | 2             | 2      |     0.8959 |      0.8138 |   0.8340 |     0.8238 |   0.9608 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  28 | 3             | 2      |     0.9779 |      1.0000 |   0.3219 |     0.4870 |   0.9996 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  29 | 4             | 2      |     0.0282 |      0.0000 |   0.0000 |     0.0000 |   0.9337 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  30 | 5             | 2      |     0.9186 |      0.8908 |   0.9120 |     0.9013 |   0.9843 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  31 | 6             | 2      |     0.9485 |      0.9546 |   0.9242 |     0.9391 |   0.9900 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  32 | 7             | 2      |     1.0000 |      0.0000 |   0.0000 |     0.0000 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  33 | 8             | 2      |     0.9405 |      0.9897 |   0.9026 |     0.9441 |   0.9964 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  34 | 9             | 2      |     0.9981 |      0.9959 |   1.0000 |     0.9980 |   0.9998 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  35 | 10            | 2      |     0.9654 |      0.9887 |   0.9376 |     0.9624 |   0.9941 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  36 | 11            | 2      |     0.9524 |      0.9705 |   0.9211 |     0.9452 |   0.9922 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  37 | 12            | 2      |     0.9359 |      0.8847 |   0.9598 |     0.9207 |   0.9903 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  38 | 13            | 2      |     0.9858 |      0.9535 |   0.9991 |     0.9758 |   0.9998 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  39 | 14            | 2      |     1.0000 |      1.0000 |   1.0000 |     1.0000 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  40 | 15            | 2      |     1.0000 |      1.0000 |   1.0000 |     1.0000 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  41 | 16            | 2      |     0.9203 |      1.0000 |   0.9203 |     0.9585 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  42 | 17            | 2      |     0.9883 |      0.9800 |   0.9981 |     0.9889 |   0.9994 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  43 | 18            | 2      |     0.9615 |      0.9626 |   0.9638 |     0.9632 |   0.9942 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  44 | 19            | 2      |     0.8320 |      0.6227 |   1.0000 |     0.7675 |   0.9974 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  45 | 20            | 2      |     0.9209 |      0.8515 |   0.9962 |     0.9182 |   0.9957 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  46 | 21            | 2      |     0.1216 |      1.0000 |   0.1216 |     0.2168 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  47 | 22            | 2      |     0.8336 |      0.7605 |   0.9358 |     0.8391 |   0.9568 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  48 | 23            | 2      |     0.9706 |      1.0000 |   0.9702 |     0.9849 |   0.9924 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  49 | 24            | 2      |     0.6804 |      0.4605 |   0.9971 |     0.6300 |   0.9684 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  50 | 0             | 3      |     0.9671 |      0.9982 |   0.9110 |     0.9526 |   0.9965 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  51 | 1             | 3      |     0.8473 |      1.0000 |   0.7136 |     0.8328 |   0.9938 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  52 | 2             | 3      |     0.8735 |      0.7954 |   0.7710 |     0.7830 |   0.9568 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  53 | 3             | 3      |     0.9727 |      0.9149 |   0.2654 |     0.4115 |   0.9971 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  54 | 4             | 3      |     0.0586 |      1.0000 |   0.0260 |     0.0507 |   0.9396 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  55 | 5             | 3      |     0.9169 |      0.9135 |   0.8796 |     0.8962 |   0.9842 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  56 | 6             | 3      |     0.9416 |      0.9637 |   0.8951 |     0.9281 |   0.9925 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  57 | 7             | 3      |     1.0000 |      0.0000 |   0.0000 |     0.0000 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  58 | 8             | 3      |     0.9289 |      0.9873 |   0.8809 |     0.9311 |   0.9943 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  59 | 9             | 3      |     0.9983 |      0.9964 |   1.0000 |     0.9982 |   1.0000 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  60 | 10            | 3      |     0.9194 |      0.9836 |   0.8402 |     0.9063 |   0.9874 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  61 | 11            | 3      |     0.9354 |      0.9919 |   0.8657 |     0.9245 |   0.9942 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  62 | 12            | 3      |     0.9641 |      0.9580 |   0.9494 |     0.9536 |   0.9906 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  63 | 13            | 3      |     0.9945 |      0.9840 |   0.9969 |     0.9904 |   0.9998 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  64 | 14            | 3      |     1.0000 |      1.0000 |   1.0000 |     1.0000 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  65 | 15            | 3      |     0.6667 |      1.0000 |   0.6667 |     0.8000 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  66 | 16            | 3      |     0.9481 |      1.0000 |   0.9481 |     0.9734 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  67 | 17            | 3      |     0.9846 |      0.9788 |   0.9921 |     0.9854 |   0.9992 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  68 | 18            | 3      |     0.9670 |      0.9821 |   0.9535 |     0.9676 |   0.9970 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  69 | 19            | 3      |     0.9105 |      0.7609 |   1.0000 |     0.8642 |   0.9959 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  70 | 20            | 3      |     0.9285 |      0.8672 |   0.9966 |     0.9274 |   0.9940 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  71 | 21            | 3      |     0.1039 |      1.0000 |   0.1039 |     0.1882 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  72 | 22            | 3      |     0.8568 |      0.7934 |   0.9458 |     0.8629 |   0.9582 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  73 | 23            | 3      |     0.9642 |      0.9844 |   0.9791 |     0.9817 |   0.9519 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  74 | 24            | 3      |     0.6880 |      0.4797 |   0.9888 |     0.6460 |   0.9391 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  75 | 0             | 4      |     0.9718 |      1.0000 |   0.9234 |     0.9602 |   0.9981 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  76 | 1             | 4      |     0.8575 |      1.0000 |   0.7324 |     0.8455 |   0.9997 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  77 | 2             | 4      |     0.8695 |      0.8093 |   0.7629 |     0.7854 |   0.9525 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  78 | 3             | 4      |     0.9566 |      0.0000 |   0.0000 |     0.0000 |   0.9798 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  79 | 4             | 4      |     0.0372 |      0.0000 |   0.0000 |     0.0000 |   0.9688 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  80 | 5             | 4      |     0.9179 |      0.9030 |   0.8978 |     0.9004 |   0.9848 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  81 | 6             | 4      |     0.9438 |      0.9545 |   0.9130 |     0.9333 |   0.9929 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  82 | 7             | 4      |     1.0000 |      0.0000 |   0.0000 |     0.0000 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  83 | 8             | 4      |     0.9267 |      0.9821 |   0.8815 |     0.9291 |   0.9949 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  84 | 9             | 4      |     0.9965 |      0.9960 |   0.9965 |     0.9963 |   0.9999 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  85 | 10            | 4      |     0.9416 |      0.9957 |   0.8811 |     0.9349 |   0.9958 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  86 | 11            | 4      |     0.9499 |      0.9762 |   0.9138 |     0.9440 |   0.9923 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  87 | 12            | 4      |     0.9566 |      0.9254 |   0.9651 |     0.9448 |   0.9945 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  88 | 13            | 4      |     0.9867 |      0.9556 |   0.9996 |     0.9771 |   0.9998 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  89 | 14            | 4      |     1.0000 |      1.0000 |   1.0000 |     1.0000 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  90 | 16            | 4      |     0.9264 |      1.0000 |   0.9264 |     0.9618 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  91 | 17            | 4      |     0.9925 |      0.9894 |   0.9964 |     0.9929 |   0.9997 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  92 | 18            | 4      |     0.9706 |      0.9714 |   0.9719 |     0.9716 |   0.9961 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  93 | 19            | 4      |     0.8237 |      0.5966 |   1.0000 |     0.7473 |   0.9958 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  94 | 20            | 4      |     0.9001 |      0.8184 |   0.9991 |     0.8998 |   0.9955 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  95 | 21            | 4      |     0.0697 |      1.0000 |   0.0697 |     0.1303 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  96 | 22            | 4      |     0.8229 |      0.7507 |   0.9312 |     0.8313 |   0.9519 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  97 | 23            | 4      |     0.9793 |      1.0000 |   0.9789 |     0.9893 |   0.9943 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  98 | 24            | 4      |     0.6897 |      0.4783 |   0.9899 |     0.6449 |   0.9449 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n|  99 | 0             | 5      |     0.9688 |      1.0000 |   0.9144 |     0.9553 |   0.9981 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 100 | 1             | 5      |     0.8632 |      1.0000 |   0.7401 |     0.8506 |   0.9996 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 101 | 2             | 5      |     0.8718 |      0.8153 |   0.7619 |     0.7877 |   0.9658 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 102 | 3             | 5      |     0.9672 |      0.0000 |   0.0000 |     0.0000 |   0.9976 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 103 | 4             | 5      |     0.0363 |      0.0000 |   0.0000 |     0.0000 |   0.9121 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 104 | 5             | 5      |     0.9192 |      0.8987 |   0.8987 |     0.8987 |   0.9861 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 105 | 6             | 5      |     0.9373 |      0.9515 |   0.8987 |     0.9243 |   0.9921 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 106 | 7             | 5      |     1.0000 |      0.0000 |   0.0000 |     0.0000 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 107 | 8             | 5      |     0.9226 |      0.9794 |   0.8787 |     0.9263 |   0.9952 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 108 | 9             | 5      |     0.9969 |      0.9971 |   0.9966 |     0.9969 |   0.9998 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 109 | 10            | 5      |     0.9384 |      0.9924 |   0.8770 |     0.9312 |   0.9811 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 110 | 11            | 5      |     0.9631 |      0.9925 |   0.9250 |     0.9576 |   0.9951 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 111 | 12            | 5      |     0.9440 |      0.9049 |   0.9557 |     0.9296 |   0.9883 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 112 | 13            | 5      |     0.9922 |      0.9738 |   0.9989 |     0.9862 |   0.9999 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 113 | 14            | 5      |     1.0000 |      1.0000 |   1.0000 |     1.0000 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 114 | 15            | 5      |     1.0000 |      1.0000 |   1.0000 |     1.0000 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 115 | 16            | 5      |     0.9246 |      1.0000 |   0.9246 |     0.9608 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 116 | 17            | 5      |     0.9824 |      0.9727 |   0.9946 |     0.9835 |   0.9984 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 117 | 18            | 5      |     0.9718 |      0.9813 |   0.9651 |     0.9731 |   0.9966 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 118 | 19            | 5      |     0.8780 |      0.6868 |   1.0000 |     0.8143 |   0.9940 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 119 | 20            | 5      |     0.9074 |      0.8302 |   1.0000 |     0.9072 |   0.9943 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 120 | 21            | 5      |     0.0468 |      1.0000 |   0.0468 |     0.0893 | nan      |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 121 | 22            | 5      |     0.8448 |      0.7823 |   0.9303 |     0.8499 |   0.9533 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 122 | 23            | 5      |     0.9839 |      1.0000 |   0.9837 |     0.9918 |   0.9931 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 123 | 24            | 5      |     0.6835 |      0.4681 |   0.9990 |     0.6375 |   0.9445 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n| 124 | Average       | -      |     0.8623 |      0.8464 |   0.8013 |     0.7923 |   0.9854 |\n+-----+---------------+--------+------------+-------------+----------+------------+----------+\n‚úÖ Hasil evaluasi per participant disimpan dalam 'hasil_evaluasi_per_participant.csv'\n","output_type":"stream"}],"execution_count":3}]}